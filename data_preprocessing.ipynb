{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8013e5",
   "metadata": {},
   "source": [
    "# Structured Data\n",
    "## Structured Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# --- Project and output directory configuration ---\n",
    "# Use environment variable or default placeholder for PROJECT_ID\n",
    "PROJECT_ID = os.getenv('MIMIC_PROJECT_ID', 'your-project-id-here')\n",
    "OUTPUT_DIR = \"mimiciv_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def get_eligible_subject_ids():\n",
    "    \"\"\"\n",
    "    Retrieve subject_ids based on anchor_year_group criteria.\n",
    "    Returns a list of subject_ids for patients with anchor_year_group \n",
    "    in 2017-2019 and earlier years.\n",
    "    \"\"\"\n",
    "    sql_query = \"\"\"\n",
    "    -- Select patients with anchor_year_group in 2017-2019 and earlier\n",
    "    WITH eligible_patients AS (\n",
    "      SELECT\n",
    "        subject_id,\n",
    "        anchor_year,\n",
    "        anchor_year_group\n",
    "      FROM\n",
    "        `physionet-data.mimiciv_3_1_hosp.patients`\n",
    "      WHERE\n",
    "        anchor_year_group IN ('2008 - 2010', '2011 - 2013', '2014 - 2016', '2017 - 2019')\n",
    "    )\n",
    "    \n",
    "    -- Get admissions records for these patients\n",
    "    SELECT DISTINCT\n",
    "      a.subject_id\n",
    "    FROM\n",
    "      `physionet-data.mimiciv_3_1_hosp.admissions` AS a\n",
    "    JOIN\n",
    "      eligible_patients AS p\n",
    "    ON\n",
    "      a.subject_id = p.subject_id\n",
    "    ORDER BY\n",
    "      a.subject_id\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing subject_id selection query...\")\n",
    "    df_result = client.query(sql_query).to_dataframe()\n",
    "    subject_ids = df_result['subject_id'].astype(int).tolist()\n",
    "    \n",
    "    # Save the selected subject_ids to CSV for reference\n",
    "    subject_ids_path = os.path.join(OUTPUT_DIR, \"selected_subject_ids.csv\")\n",
    "    df_result.to_csv(subject_ids_path, index=False)\n",
    "    print(f\"Selected subject_ids saved to {subject_ids_path}\")\n",
    "    \n",
    "    return subject_ids\n",
    "\n",
    "def query_table(dataset, table, subject_ids=None, limit=None):\n",
    "    \"\"\"\n",
    "    Fetch all records belonging to subject_ids from a single table and write to CSV.\n",
    "    If subject_ids=None, fetch the entire table.\n",
    "    If the corresponding CSV already exists locally, skip fetching.\n",
    "    \"\"\"\n",
    "    fname = f\"{dataset.split('.')[-1]}_{table}.csv\"\n",
    "    path = os.path.join(OUTPUT_DIR, fname)\n",
    "    \n",
    "    # Skip download if CSV file already exists\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"Skipping {table}, local file already exists {path}\")\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    \n",
    "    # Otherwise, download from BigQuery\n",
    "    sql = f\"SELECT * FROM `{dataset}.{table}`\"\n",
    "    if subject_ids is not None:\n",
    "        sql += \"\\nWHERE subject_id IN UNNEST(@subject_ids)\"\n",
    "    if limit:\n",
    "        sql += f\"\\nLIMIT {limit}\"\n",
    "\n",
    "    job_config = None\n",
    "    if subject_ids is not None:\n",
    "        job_config = bigquery.QueryJobConfig(\n",
    "            query_parameters=[\n",
    "                bigquery.ArrayQueryParameter(\"subject_ids\", \"INT64\", subject_ids)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    df = client.query(sql, job_config=job_config).to_dataframe()\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved {table} → {path} ({df.shape[0]} rows)\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # 1) Get eligible subject_ids using SQL criteria\n",
    "    subject_ids = get_eligible_subject_ids()\n",
    "    print(f\"Selected {len(subject_ids)} subject_ids based on anchor_year_group criteria\")\n",
    "    \n",
    "    # 2) Batch fetch structured tables\n",
    "    hosp_dataset = \"physionet-data.mimiciv_3_1_hosp\"\n",
    "    hosp_tables = [\n",
    "        \"admissions\",\n",
    "        \"patients\", \n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"drgcodes\",\n",
    "        \"labevents\",\n",
    "        \"microbiologyevents\",\n",
    "        \"prescriptions\",\n",
    "        \"emar\",\n",
    "        \"services\",\n",
    "        \"transfers\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nFetching hospital-related tables...\")\n",
    "    for t in hosp_tables:\n",
    "        query_table(hosp_dataset, t, subject_ids)\n",
    "\n",
    "    # 3) Batch fetch ICU-related tables\n",
    "    icu_dataset = \"physionet-data.mimiciv_3_1_icu\"\n",
    "    icu_tables = [\n",
    "        \"icustays\",\n",
    "        \"chartevents\",\n",
    "        \"datetimeevents\",\n",
    "        \"inputevents\",\n",
    "        \"outputevents\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nFetching ICU-related tables...\")\n",
    "    for t in icu_tables:\n",
    "        query_table(icu_dataset, t, subject_ids)\n",
    "\n",
    "    print(\"\\nAll structured data has been fetched successfully.\")\n",
    "    print(f\"Total number of patients processed: {len(subject_ids)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c2541",
   "metadata": {},
   "source": [
    "## Dictionary Table Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# ——— Project and output directory configuration ———\n",
    "PROJECT_ID = os.getenv('MIMIC_PROJECT_ID', 'your-project-id-here')\n",
    "OUTPUT_DIR = \"mimiciv_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Read subject_id from local file\n",
    "ids_path = os.path.join(OUTPUT_DIR, \"selected_subject_ids.csv\")\n",
    "if not os.path.exists(ids_path):\n",
    "    print(f\"ERROR: Unable to find {ids_path}\")\n",
    "    sys.exit(1)\n",
    "subject_ids = pd.read_csv(ids_path)['subject_id'].dropna().astype(int).tolist()\n",
    "print(f\"Loaded {len(subject_ids)} subject_id\")\n",
    "\n",
    "# Function specifically for extracting dictionary tables\n",
    "def extract_dict_table(table_name):\n",
    "    \"\"\"\n",
    "    If mimiciv_3_1_hosp_{table_name}.csv already exists locally, skip download;\n",
    "    Otherwise, read from BigQuery and save.\n",
    "    \"\"\"\n",
    "    fname = f\"mimiciv_3_1_hosp_{table_name}.csv\"\n",
    "    path = os.path.join(OUTPUT_DIR, fname)\n",
    "\n",
    "    # Skip download if local file already exists\n",
    "    if os.path.isfile(path):\n",
    "        print(f\"Skipping {table_name}, local file already exists {path}\")\n",
    "        return\n",
    "\n",
    "    # Otherwise, read from BigQuery and save\n",
    "    sql = f\"SELECT * FROM `physionet-data.mimiciv_3_1_hosp.{table_name}`\"\n",
    "    try:\n",
    "        df = client.query(sql).to_dataframe()\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Saved dictionary table {table_name} → {path} ({df.shape[0]} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {table_name}: {e}\")\n",
    "\n",
    "# Correct dictionary table name list\n",
    "dict_tables = [\n",
    "    \"d_icd_diagnoses\",\n",
    "    \"d_icd_procedures\",\n",
    "    \"d_hcpcs\"\n",
    "]\n",
    "\n",
    "for t in dict_tables:\n",
    "    extract_dict_table(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7dc78",
   "metadata": {},
   "source": [
    "## Data Consolidation and Data Leakage Prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = 'mimiciv_data'\n",
    "\n",
    "# 1. Read patient ID list (updated to use the new selection file)\n",
    "subjects = pd.read_csv(os.path.join(DATA_DIR, 'selected_subject_ids.csv'))\n",
    "print(f\"Loaded {len(subjects)} subjects from SQL-based selection criteria\")\n",
    "\n",
    "# Validate that we have the expected column structure\n",
    "assert 'subject_id' in subjects.columns, \"subject_id column not found in selected subjects file\"\n",
    "assert len(subjects) > 0, \"No subjects found in selection file\"\n",
    "\n",
    "# 2. Load admissions, calculate index admission\n",
    "adm = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_admissions.csv'),\n",
    "    parse_dates=['admittime','dischtime', 'deathtime']\n",
    ")\n",
    "adm['los'] = (adm['dischtime'] - adm['admittime']).dt.total_seconds() / 86400\n",
    "\n",
    "# Sort by admission time, take the first record for each subject as index admission\n",
    "index_adm = adm.sort_values(['subject_id','admittime']).groupby('subject_id', as_index=False).first()\n",
    "\n",
    "# Build index admission features\n",
    "idx_feat = index_adm.set_index('subject_id')[[\n",
    "    'admission_type','insurance','race','los',\n",
    "    'admission_location','discharge_location','hospital_expire_flag'\n",
    "]].rename(columns=lambda c: f'idx_{c}')\n",
    "\n",
    "# 3. Derive new labels: 30-day readmission, ICU during readmission, death after readmission\n",
    "\n",
    "# 3.1 30-day readmission after index admission\n",
    "def calculate_readmission_labels(adm_df, index_adm_df):\n",
    "    \"\"\"Calculate 30-day readmission labels based on index admission\"\"\"\n",
    "    # Create a mapping of subject_id to index discharge time\n",
    "    index_discharge = index_adm_df.set_index('subject_id')['dischtime']\n",
    "    \n",
    "    # Find all admissions after index admission for each patient\n",
    "    adm_with_index = adm_df.merge(\n",
    "        index_discharge.reset_index().rename(columns={'dischtime': 'index_dischtime'}),\n",
    "        on='subject_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Filter to admissions after index discharge\n",
    "    subsequent_adm = adm_with_index[\n",
    "        adm_with_index['admittime'] > adm_with_index['index_dischtime']\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate days from index discharge to subsequent admission\n",
    "    subsequent_adm['days_from_index_discharge'] = (\n",
    "        subsequent_adm['admittime'] - subsequent_adm['index_dischtime']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Identify 30-day readmissions\n",
    "    readmit_30d = subsequent_adm[subsequent_adm['days_from_index_discharge'] <= 30]\n",
    "    \n",
    "    # Create binary label for each subject\n",
    "    readmit_subjects = readmit_30d['subject_id'].unique()\n",
    "    readmit_labels = pd.Series(\n",
    "        index=index_adm_df['subject_id'],\n",
    "        data=0,\n",
    "        name='readmission_30d'\n",
    "    )\n",
    "    readmit_labels.loc[readmit_subjects] = 1\n",
    "    \n",
    "    return readmit_labels, subsequent_adm\n",
    "\n",
    "# Calculate readmission labels\n",
    "readmit_lbl, subsequent_admissions = calculate_readmission_labels(adm, index_adm)\n",
    "\n",
    "# 3.2 ICU admission during readmission episodes\n",
    "def calculate_icu_during_readmission(subsequent_adm_df, icu_df):\n",
    "    \"\"\"Calculate ICU admission during readmission episodes\"\"\"\n",
    "    if len(subsequent_adm_df) == 0:\n",
    "        return pd.Series(index=subjects['subject_id'], data=0, name='icu_during_readmission')\n",
    "    \n",
    "    # Get hadm_ids for all subsequent admissions (readmissions)\n",
    "    readmission_hadm_ids = set(subsequent_adm_df['hadm_id'].unique())\n",
    "    \n",
    "    # Filter ICU stays to those occurring during readmission episodes\n",
    "    icu_during_readmission = icu_df[icu_df['hadm_id'].isin(readmission_hadm_ids)]\n",
    "    \n",
    "    # Identify subjects with ICU stays during readmissions\n",
    "    icu_readmit_subjects = icu_during_readmission['subject_id'].unique()\n",
    "    \n",
    "    # Create binary labels\n",
    "    icu_readmit_labels = pd.Series(\n",
    "        index=subjects['subject_id'],\n",
    "        data=0,\n",
    "        name='icu_during_readmission'\n",
    "    )\n",
    "    icu_readmit_labels.loc[icu_readmit_subjects] = 1\n",
    "    \n",
    "    return icu_readmit_labels\n",
    "\n",
    "# Load ICU stays data\n",
    "icu = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_icustays.csv'))\n",
    "icu_readmit_lbl = calculate_icu_during_readmission(subsequent_admissions, icu)\n",
    "\n",
    "# 3.3 Death after readmission\n",
    "def calculate_death_after_readmission(subsequent_adm_df):\n",
    "    \"\"\"Calculate death during readmission episodes\"\"\"\n",
    "    if len(subsequent_adm_df) == 0:\n",
    "        return pd.Series(index=subjects['subject_id'], data=0, name='death_after_readmission')\n",
    "    \n",
    "    # Find readmissions that resulted in death (hospital_expire_flag = 1)\n",
    "    death_during_readmission = subsequent_adm_df[\n",
    "        subsequent_adm_df['hospital_expire_flag'] == 1\n",
    "    ]\n",
    "    \n",
    "    # Identify subjects who died during readmission\n",
    "    death_readmit_subjects = death_during_readmission['subject_id'].unique()\n",
    "    \n",
    "    # Create binary labels\n",
    "    death_readmit_labels = pd.Series(\n",
    "        index=subjects['subject_id'],\n",
    "        data=0,\n",
    "        name='death_after_readmission'\n",
    "    )\n",
    "    death_readmit_labels.loc[death_readmit_subjects] = 1\n",
    "    \n",
    "    return death_readmit_labels\n",
    "\n",
    "death_readmit_lbl = calculate_death_after_readmission(subsequent_admissions)\n",
    "\n",
    "# Combine all labels\n",
    "labels = pd.concat([\n",
    "    readmit_lbl.rename('readmission_30d'),\n",
    "    icu_readmit_lbl,\n",
    "    death_readmit_lbl\n",
    "], axis=1).fillna(0).astype(int)\n",
    "\n",
    "print(f\"Label distribution:\")\n",
    "print(f\"30-day readmission: {labels['readmission_30d'].sum()} / {len(labels)} ({labels['readmission_30d'].mean():.3f})\")\n",
    "print(f\"ICU during readmission: {labels['icu_during_readmission'].sum()} / {len(labels)} ({labels['icu_during_readmission'].mean():.3f})\")\n",
    "print(f\"Death after readmission: {labels['death_after_readmission'].sum()} / {len(labels)} ({labels['death_after_readmission'].mean():.3f})\")\n",
    "\n",
    "# 4. Population features (patients)\n",
    "pat = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_patients.csv')).set_index('subject_id')\n",
    "pat_feat = pd.get_dummies(\n",
    "    pat[['gender','anchor_age','anchor_year_group']],\n",
    "    columns=['gender','anchor_year_group'], dummy_na=True\n",
    ")\n",
    "\n",
    "# 5. Prepare (subject_id, hadm_id) pairs for index admission to use for filtering\n",
    "idx_pairs = index_adm[['subject_id','hadm_id']]\n",
    "\n",
    "# 6. Perform inner merge on detail tables by idx_pairs, then aggregate by subject_id\n",
    "dfs = [idx_feat, labels, pat_feat]\n",
    "\n",
    "def agg_by_index(df, id_col, agg_specs):\n",
    "    \"\"\"\n",
    "    Aggregate data from detail tables by index admission\n",
    "    df: Original detail table DataFrame\n",
    "    id_col: Column name for hadm_id\n",
    "    agg_specs: dict of output_col: (col, func)\n",
    "    \"\"\"\n",
    "    tmp = df.merge(idx_pairs, on=['subject_id', id_col], how='inner')\n",
    "    return tmp.groupby('subject_id').agg(**agg_specs)\n",
    "\n",
    "# 6.1 Diagnoses ICD\n",
    "dx = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_diagnoses_icd.csv'))\n",
    "dx_feat = agg_by_index(dx, 'hadm_id', {\n",
    "    'idx_dx_count': ('icd_code','count'),\n",
    "    'idx_unique_dx': ('icd_code','nunique')\n",
    "})\n",
    "dfs.append(dx_feat)\n",
    "\n",
    "# 6.2 Procedures ICD\n",
    "pr = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_procedures_icd.csv'))\n",
    "pr_feat = agg_by_index(pr, 'hadm_id', {\n",
    "    'idx_proc_count': ('icd_code','count'),\n",
    "    'idx_unique_proc': ('icd_code','nunique')\n",
    "})\n",
    "dfs.append(pr_feat)\n",
    "\n",
    "# 6.3 DRG codes\n",
    "drg = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_drgcodes.csv'))\n",
    "drg['drg_code'] = drg['drg_code'].astype(str)\n",
    "dr_pairs = idx_pairs.copy()\n",
    "dr = drg.merge(dr_pairs, on=['subject_id','hadm_id'], how='inner')\n",
    "dr_feat = dr.groupby('subject_id').agg(\n",
    "    idx_drg_count=('drg_code','count'),\n",
    "    idx_severity_mode=('drg_severity', lambda x: x.mode().iat[0] if not x.mode().empty else pd.NA)\n",
    ")\n",
    "dfs.append(dr_feat)\n",
    "\n",
    "# 6.4 Laboratory events\n",
    "lab = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_labevents.csv'))\n",
    "lab['abnormal'] = lab['flag'].fillna('NORMAL') != 'NORMAL'\n",
    "lab_feat = agg_by_index(lab, 'hadm_id', {\n",
    "    'idx_lab_count': ('labevent_id','count'),\n",
    "    'idx_lab_abn': ('abnormal','sum')\n",
    "})\n",
    "dfs.append(lab_feat)\n",
    "\n",
    "# 6.5 Microbiology events\n",
    "micro = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_microbiologyevents.csv'))\n",
    "micro['pos'] = micro['interpretation'] == 'Positive'\n",
    "micro_feat = agg_by_index(micro, 'hadm_id', {\n",
    "    'idx_micro_count': ('microevent_id','count'),\n",
    "    'idx_micro_pos': ('pos','sum')\n",
    "})\n",
    "dfs.append(micro_feat)\n",
    "\n",
    "# 6.6 Prescriptions and EMAR\n",
    "pres = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_prescriptions.csv'), low_memory=False)\n",
    "emar = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_emar.csv'))\n",
    "med = pd.concat([\n",
    "    pres[['subject_id','hadm_id','poe_id','drug_type']],\n",
    "    emar[['subject_id','hadm_id','poe_id','event_txt']].rename(columns={'event_txt':'drug_type'})\n",
    "], ignore_index=True)\n",
    "med['abx'] = med['drug_type'].str.contains('ANTIBIOTIC', na=False)\n",
    "med_feat = agg_by_index(med, 'hadm_id', {\n",
    "    'idx_med_count': ('poe_id','count'),\n",
    "    'idx_med_abx': ('abx','sum')\n",
    "})\n",
    "dfs.append(med_feat)\n",
    "\n",
    "# 6.7 Services and transfers\n",
    "serv = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_services.csv'))\n",
    "serv_feat = agg_by_index(serv, 'hadm_id', {\n",
    "    'idx_serv_changes': ('transfertime','count'),\n",
    "    'idx_uniq_serv': ('curr_service','nunique')\n",
    "})\n",
    "trans = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_hosp_transfers.csv'))\n",
    "trans_feat = agg_by_index(trans, 'hadm_id', {\n",
    "    'idx_trans_count': ('transfer_id','count')\n",
    "})\n",
    "dfs.extend([serv_feat, trans_feat])\n",
    "\n",
    "# 6.8 ICU stays during index admission\n",
    "icu_events = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_icustays.csv'))\n",
    "icu_feat = agg_by_index(icu_events, 'hadm_id', {\n",
    "    'idx_icu_flag': ('stay_id', lambda s: 1),\n",
    "    'idx_icu_los': ('los','sum')\n",
    "})\n",
    "dfs.append(icu_feat)\n",
    "\n",
    "# 6.9 Chart events\n",
    "char = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_chartevents.csv'))\n",
    "char_feat = agg_by_index(char, 'hadm_id', {\n",
    "    'idx_char_count': ('stay_id','count')\n",
    "})\n",
    "dfs.append(char_feat)\n",
    "\n",
    "# 6.10 Datetime events\n",
    "dt = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_datetimeevents.csv'))\n",
    "dt_feat = agg_by_index(dt, 'hadm_id', {\n",
    "    'idx_dt_count': ('itemid','count')\n",
    "})\n",
    "dfs.append(dt_feat)\n",
    "\n",
    "# 6.11 Fluid balance\n",
    "inp = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_inputevents.csv'))\n",
    "out = pd.read_csv(os.path.join(DATA_DIR, 'mimiciv_3_1_icu_outputevents.csv'))\n",
    "inp_feat = agg_by_index(inp, 'hadm_id', {'idx_in_vol': ('totalamount','sum')})\n",
    "out_feat = agg_by_index(out, 'hadm_id', {'idx_out_vol': ('value','sum')})\n",
    "fluids = inp_feat.join(out_feat, how='outer').fillna(0)\n",
    "fluids['idx_net_fluid'] = fluids['idx_in_vol'] - fluids['idx_out_vol']\n",
    "dfs.append(fluids)\n",
    "\n",
    "# 7. Merge all features and labels\n",
    "features = subjects.set_index('subject_id').join(dfs, how='left').fillna(0)\n",
    "\n",
    "# Handle any remaining object columns that should be numeric\n",
    "features = features.infer_objects(copy=False)\n",
    "\n",
    "# 8. Output\n",
    "out_file = os.path.join(DATA_DIR, 'patient_features_noleak.csv')\n",
    "features.reset_index().to_csv(out_file, index=False)\n",
    "\n",
    "print(f'\\nGenerated no-leak feature file: {out_file}')\n",
    "print(f'Final dataset shape: {features.shape}')\n",
    "print(f'\\nFinal label distribution:')\n",
    "for label_col in ['readmission_30d', 'icu_during_readmission', 'death_after_readmission']:\n",
    "    if label_col in features.columns:\n",
    "        count = features[label_col].sum()\n",
    "        rate = features[label_col].mean()\n",
    "        print(f'{label_col}: {count} / {len(features)} ({rate:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff23908",
   "metadata": {},
   "source": [
    "# Notes Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from typing import List, Optional, Dict\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ClinicalNotesEmbedding:\n",
    "    \"\"\"\n",
    "    Advanced clinical notes embedding generator using domain-specific models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"emilyalsentzer/Bio_ClinicalBERT\", \n",
    "                 max_length: int = 512, batch_size: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize the embedding generator with clinical domain model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Pre-trained clinical model name\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "            batch_size: Batch size for processing\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Loading clinical model: {model_name}\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "    \n",
    "    def preprocess_clinical_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced preprocessing for clinical notes\n",
    "        \n",
    "        Args:\n",
    "            text: Raw clinical note text\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed clinical text\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove common artifacts and standardize format\n",
    "        text = str(text).strip()\n",
    "        \n",
    "        # Remove excessive whitespace and line breaks\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove common template artifacts\n",
    "        text = re.sub(r'\\[Report de-identified.*?\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\*\\*[^*]*\\*\\*', '', text)  # Remove **enclosed** text\n",
    "        \n",
    "        # Standardize medical abbreviations spacing\n",
    "        text = re.sub(r'([a-zA-Z])\\.([a-zA-Z])', r'\\1. \\2', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        text = re.sub(r'[-]{3,}', '---', text)\n",
    "        \n",
    "        # Truncate very long texts while preserving sentence structure\n",
    "        if len(text) > self.max_length * 4:  # Rough character limit\n",
    "            sentences = text.split('. ')\n",
    "            truncated_sentences = []\n",
    "            char_count = 0\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if char_count + len(sentence) < self.max_length * 3:\n",
    "                    truncated_sentences.append(sentence)\n",
    "                    char_count += len(sentence)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            text = '. '.join(truncated_sentences)\n",
    "            if not text.endswith('.'):\n",
    "                text += '.'\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def get_embeddings_batch(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a batch of texts using advanced pooling strategy\n",
    "        \n",
    "        Args:\n",
    "            texts: List of preprocessed clinical texts\n",
    "            \n",
    "        Returns:\n",
    "            Array of embeddings with shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        if not texts or all(not text.strip() for text in texts):\n",
    "            return np.zeros((len(texts), 768))  # ClinicalBERT embedding dimension\n",
    "        \n",
    "        # Tokenize with proper handling of long sequences\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Advanced pooling strategy: weighted average of last 4 layers\n",
    "            # This captures both local and global semantic information\n",
    "            hidden_states = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            \n",
    "            # Apply attention mask to ignore padding tokens\n",
    "            masked_hidden_states = hidden_states * attention_mask.unsqueeze(-1)\n",
    "            \n",
    "            # Calculate mean pooling with attention weighting\n",
    "            sum_embeddings = torch.sum(masked_hidden_states, dim=1)\n",
    "            sum_mask = torch.sum(attention_mask, dim=1, keepdim=True)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            # Apply layer normalization for stability\n",
    "            embeddings = torch.nn.functional.normalize(mean_embeddings, p=2, dim=1)\n",
    "            \n",
    "        return embeddings.cpu().numpy()\n",
    "    \n",
    "    def aggregate_note_embeddings(self, embeddings_list: List[np.ndarray], \n",
    "                                  strategy: str = \"weighted_mean\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Aggregate multiple note embeddings for a single patient\n",
    "        \n",
    "        Args:\n",
    "            embeddings_list: List of embedding arrays for individual notes\n",
    "            strategy: Aggregation strategy (\"weighted_mean\", \"max_pool\", \"attention\")\n",
    "            \n",
    "        Returns:\n",
    "            Single aggregated embedding\n",
    "        \"\"\"\n",
    "        if not embeddings_list:\n",
    "            return np.zeros(768)  # Return zero vector for empty list\n",
    "        \n",
    "        if len(embeddings_list) == 1:\n",
    "            return embeddings_list[0]\n",
    "        \n",
    "        embeddings_array = np.array(embeddings_list)\n",
    "        \n",
    "        if strategy == \"weighted_mean\":\n",
    "            # Weight by embedding magnitude (represents information content)\n",
    "            weights = np.linalg.norm(embeddings_array, axis=1)\n",
    "            weights = weights / (np.sum(weights) + 1e-9)\n",
    "            return np.average(embeddings_array, weights=weights, axis=0)\n",
    "        \n",
    "        elif strategy == \"max_pool\":\n",
    "            return np.max(embeddings_array, axis=0)\n",
    "        \n",
    "        else:  # Default to simple mean\n",
    "            return np.mean(embeddings_array, axis=0)\n",
    "    \n",
    "    def process_notes(self, notes_df: pd.DataFrame, note_type: str) -> Dict[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Process all notes of a specific type and generate patient-level embeddings\n",
    "        \n",
    "        Args:\n",
    "            notes_df: DataFrame containing clinical notes\n",
    "            note_type: Type of notes being processed (\"discharge\" or \"radiology\")\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping subject_id to aggregated embedding\n",
    "        \"\"\"\n",
    "        print(f\"Processing {note_type} notes...\")\n",
    "        \n",
    "        if notes_df.empty:\n",
    "            print(f\"No {note_type} notes found\")\n",
    "            return {}\n",
    "        \n",
    "        # Group notes by subject_id\n",
    "        subject_embeddings = {}\n",
    "        \n",
    "        for subject_id, group in tqdm(notes_df.groupby('subject_id'), \n",
    "                                      desc=f\"Processing {note_type} notes\"):\n",
    "            \n",
    "            # Preprocess all notes for this subject\n",
    "            texts = [self.preprocess_clinical_text(text) for text in group['text']]\n",
    "            \n",
    "            # Filter out empty texts\n",
    "            valid_texts = [text for text in texts if text.strip()]\n",
    "            \n",
    "            if not valid_texts:\n",
    "                # No valid text for this subject\n",
    "                subject_embeddings[subject_id] = np.zeros(768)\n",
    "                continue\n",
    "            \n",
    "            # Process in batches\n",
    "            all_embeddings = []\n",
    "            for i in range(0, len(valid_texts), self.batch_size):\n",
    "                batch_texts = valid_texts[i:i + self.batch_size]\n",
    "                batch_embeddings = self.get_embeddings_batch(batch_texts)\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Aggregate embeddings for this subject\n",
    "            if all_embeddings:\n",
    "                aggregated_embedding = self.aggregate_note_embeddings(all_embeddings)\n",
    "                subject_embeddings[subject_id] = aggregated_embedding\n",
    "            else:\n",
    "                subject_embeddings[subject_id] = np.zeros(768)\n",
    "        \n",
    "        print(f\"Completed processing {len(subject_embeddings)} subjects for {note_type} notes\")\n",
    "        return subject_embeddings\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to generate patient-level text embeddings\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_DIR = 'mimiciv_data'\n",
    "    OUTPUT_FILE = os.path.join(DATA_DIR, 'patient_text_embeddings_nan.csv')\n",
    "    \n",
    "    # Initialize embedding generator with advanced clinical model\n",
    "    # Alternative high-quality models:\n",
    "    # - \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    # - \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "    # - \"allenai/scibert_scivocab_uncased\"\n",
    "    embedder = ClinicalNotesEmbedding(\n",
    "        model_name=\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "        max_length=512,\n",
    "        batch_size=8  # Adjust based on GPU memory\n",
    "    )\n",
    "    \n",
    "    # Load selected subject IDs\n",
    "    print(\"Loading selected subject IDs...\")\n",
    "    selected_subjects_df = pd.read_csv(os.path.join(DATA_DIR, 'selected_subject_ids.csv'))\n",
    "    selected_subject_ids = set(selected_subjects_df['subject_id'].astype(int))\n",
    "    print(f\"Found {len(selected_subject_ids)} selected subjects\")\n",
    "    \n",
    "    # Load and filter discharge notes\n",
    "    print(\"Loading discharge notes...\")\n",
    "    try:\n",
    "        discharge_df = pd.read_csv(os.path.join(DATA_DIR, 'discharge.csv'))\n",
    "        discharge_df = discharge_df[discharge_df['subject_id'].isin(selected_subject_ids)]\n",
    "        print(f\"Loaded {len(discharge_df)} discharge notes for selected subjects\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"discharge.csv not found, creating empty DataFrame\")\n",
    "        discharge_df = pd.DataFrame()\n",
    "    \n",
    "    # Load and filter radiology notes\n",
    "    print(\"Loading radiology notes...\")\n",
    "    try:\n",
    "        radiology_df = pd.read_csv(os.path.join(DATA_DIR, 'radiology.csv'))\n",
    "        radiology_df = radiology_df[radiology_df['subject_id'].isin(selected_subject_ids)]\n",
    "        print(f\"Loaded {len(radiology_df)} radiology notes for selected subjects\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"radiology.csv not found, creating empty DataFrame\")\n",
    "        radiology_df = pd.DataFrame()\n",
    "    \n",
    "    # Process discharge notes\n",
    "    discharge_embeddings = embedder.process_notes(discharge_df, \"discharge\")\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Process radiology notes\n",
    "    radiology_embeddings = embedder.process_notes(radiology_df, \"radiology\")\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    print(\"Creating final embedding dataset...\")\n",
    "    results = []\n",
    "    \n",
    "    for subject_id in selected_subject_ids:\n",
    "        # Get discharge embedding\n",
    "        discharge_emb = discharge_embeddings.get(subject_id)\n",
    "        if discharge_emb is not None and np.any(discharge_emb != 0):\n",
    "            discharge_emb_str = json.dumps(discharge_emb.tolist())\n",
    "        else:\n",
    "            discharge_emb_str = np.nan\n",
    "        \n",
    "        # Get radiology embedding\n",
    "        radiology_emb = radiology_embeddings.get(subject_id)\n",
    "        if radiology_emb is not None and np.any(radiology_emb != 0):\n",
    "            radiology_emb_str = json.dumps(radiology_emb.tolist())\n",
    "        else:\n",
    "            radiology_emb_str = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'subject_id': subject_id,\n",
    "            'discharge_embedding': discharge_emb_str,\n",
    "            'radiology_embedding': radiology_emb_str\n",
    "        })\n",
    "    \n",
    "    # Create final DataFrame and save\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df = final_df.sort_values('subject_id').reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    discharge_valid = final_df['discharge_embedding'].notna().sum()\n",
    "    radiology_valid = final_df['radiology_embedding'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nEmbedding generation completed!\")\n",
    "    print(f\"Output saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"Total subjects: {len(final_df)}\")\n",
    "    print(f\"Subjects with discharge embeddings: {discharge_valid} ({discharge_valid/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"Subjects with radiology embeddings: {radiology_valid} ({radiology_valid/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"Subjects with both embeddings: {((final_df['discharge_embedding'].notna()) & (final_df['radiology_embedding'].notna())).sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb59da1",
   "metadata": {},
   "source": [
    "# CXR Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c42efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# Configuration Section\n",
    "# ========================================\n",
    "SUBJECT_IDS_CSV = 'mimiciv_data/selected_subject_ids.csv'\n",
    "EMB_BASE_DIR = 'generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/files'\n",
    "INTERMEDIATE_CSV = 'cxr_embeddings_by_file.csv'\n",
    "OUTPUT_CSV = 'cxr_embeddings_aggregated.csv'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========================================\n",
    "# CXR Embedding Extraction Functions\n",
    "# ========================================\n",
    "\n",
    "def subject_folder(sid: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate the embedding directory path for a given subject_id\n",
    "    \n",
    "    Args:\n",
    "        sid: Subject ID\n",
    "        \n",
    "    Returns:\n",
    "        Directory path containing the subject's embedding files\n",
    "    \"\"\"\n",
    "    prefix = sid // 1_000_000\n",
    "    return os.path.join(EMB_BASE_DIR, f'p{prefix}', f'p{sid}')\n",
    "\n",
    "def read_embedding_from_tfrecord(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read embedding vector from a single TFRecord file\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the TFRecord file\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array containing the embedding vector, or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ds = tf.data.TFRecordDataset([path])\n",
    "        feature_spec = {'embedding': tf.io.VarLenFeature(tf.float32)}\n",
    "        for raw in ds.take(1):\n",
    "            ex = tf.io.parse_single_example(raw, feature_spec)\n",
    "            return tf.sparse.to_dense(ex['embedding']).numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_subject_embeddings(subject_ids: List[int]) -> Tuple[List[Tuple[int, List[List[float]]]], int]:\n",
    "    \"\"\"\n",
    "    Extract all CXR embeddings for given subjects\n",
    "    \n",
    "    Args:\n",
    "        subject_ids: List of subject IDs to process\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (subject_data_list, max_files_count)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    max_files = 0\n",
    "    \n",
    "    print(f\"Processing {len(subject_ids)} subjects for CXR embeddings...\")\n",
    "    \n",
    "    for i, sid in enumerate(subject_ids):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Progress: {i}/{len(subject_ids)} subjects processed\")\n",
    "            \n",
    "        folder = subject_folder(sid)\n",
    "        if not os.path.isdir(folder):\n",
    "            # Skip subjects without downloaded data\n",
    "            continue\n",
    "\n",
    "        # Collect all tfrecord file embeddings for this subject\n",
    "        embeddings = []\n",
    "        # Navigate through two-level subdirectory structure: files/pXX/p<sid>/<study>/*.tfrecord\n",
    "        for study in os.listdir(folder):\n",
    "            study_dir = os.path.join(folder, study)\n",
    "            if not os.path.isdir(study_dir):\n",
    "                continue\n",
    "            for filename in os.listdir(study_dir):\n",
    "                if filename.endswith('.tfrecord'):\n",
    "                    path = os.path.join(study_dir, filename)\n",
    "                    vec = read_embedding_from_tfrecord(path)\n",
    "                    if vec is not None:\n",
    "                        embeddings.append(vec.tolist())\n",
    "\n",
    "        if len(embeddings) > max_files:\n",
    "            max_files = len(embeddings)\n",
    "\n",
    "        rows.append([sid, embeddings])\n",
    "    \n",
    "    print(f\"Extraction completed. Found embeddings for {len(rows)} subjects\")\n",
    "    print(f\"Maximum number of CXR files per subject: {max_files}\")\n",
    "    \n",
    "    return rows, max_files\n",
    "\n",
    "def save_intermediate_embeddings(rows: List[Tuple[int, List[List[float]]]], max_files: int, output_path: str):\n",
    "    \"\"\"\n",
    "    Save extracted embeddings to intermediate CSV file\n",
    "    \n",
    "    Args:\n",
    "        rows: List of (subject_id, embeddings_list) tuples\n",
    "        max_files: Maximum number of files per subject\n",
    "        output_path: Path to save the CSV file\n",
    "    \"\"\"\n",
    "    # Create DataFrame with dynamic column names\n",
    "    col_names = ['subject_id'] + [f'file_{i}' for i in range(max_files)]\n",
    "    data = []\n",
    "    \n",
    "    for sid, embs in rows:\n",
    "        # Pad with None if subject has fewer tfrecord files than max_files\n",
    "        padded = embs + [None] * (max_files - len(embs))\n",
    "        data.append([sid] + padded)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "    # Convert embedding lists to string format for CSV storage\n",
    "    for c in col_names[1:]:\n",
    "        df[c] = df[c].apply(lambda x: '' if x is None else '[' + ','.join(f'{v:.6f}' for v in x) + ']')\n",
    "\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Intermediate embeddings saved to: {output_path}\")\n",
    "\n",
    "# ========================================\n",
    "# Transformer Aggregation Module\n",
    "# ========================================\n",
    "\n",
    "class TransformerAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based aggregator for multiple CXR embeddings per subject\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_dim: int, n_heads: int = 8, hidden_dim: int = 512, \n",
    "                 n_layers: int = 2, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize Transformer aggregator\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: Embedding dimension\n",
    "            n_heads: Number of attention heads\n",
    "            hidden_dim: Hidden dimension in feedforward network\n",
    "            n_layers: Number of transformer layers\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through transformer aggregator\n",
    "        \n",
    "        Args:\n",
    "            x: Input embeddings tensor (batch_size, num_files, emb_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Aggregated embedding tensor (batch_size, emb_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        cls = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = self.transformer(x)\n",
    "        cls_out = x[:, 0, :]\n",
    "        return self.norm(cls_out)\n",
    "\n",
    "# ========================================\n",
    "# Dataset Class for Aggregation\n",
    "# ========================================\n",
    "\n",
    "class CXREmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading CXR embeddings from intermediate CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the intermediate CSV file\n",
    "        \"\"\"\n",
    "        self.rows = []\n",
    "        with open(csv_path, 'r', newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                sid = row[0]\n",
    "                # Parse JSON-formatted embedding lists from CSV\n",
    "                embeds = [json.loads(cell) for cell in row[1:] if cell.strip() != '']\n",
    "                if embeds:  # Only include subjects with at least one embedding\n",
    "                    self.rows.append((sid, embeds))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[str, torch.Tensor]:\n",
    "        sid, embeds = self.rows[idx]\n",
    "        return sid, torch.tensor(embeds, dtype=torch.float32)\n",
    "\n",
    "# ========================================\n",
    "# Aggregation Pipeline\n",
    "# ========================================\n",
    "\n",
    "def aggregate_embeddings(input_csv: str, output_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    Aggregate multiple CXR embeddings per subject using Transformer\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Path to intermediate CSV with individual embeddings\n",
    "        output_csv: Path to save aggregated embeddings\n",
    "    \"\"\"\n",
    "    print(\"Starting embedding aggregation...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = CXREmbeddingDataset(input_csv)\n",
    "    if len(ds) == 0:\n",
    "        print(\"No valid embeddings found in input CSV\")\n",
    "        return\n",
    "    \n",
    "    # Infer embedding dimension from first sample\n",
    "    _, first = ds[0]\n",
    "    emb_dim = first.size(1)\n",
    "    print(f\"Detected embedding dimension: {emb_dim}\")\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize transformer aggregator\n",
    "    agg = TransformerAggregator(emb_dim=emb_dim).to(DEVICE)\n",
    "    agg.eval()\n",
    "    \n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(\"Aggregating embeddings...\")\n",
    "\n",
    "    # Process and save aggregated embeddings\n",
    "    with open(output_csv, 'w', newline='') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow([\"subject_id\", \"agg_embedding\"])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (sid, embeds) in enumerate(dl):\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Aggregation progress: {i}/{len(dl)} subjects processed\")\n",
    "                \n",
    "                embeds = embeds.to(DEVICE)  # Shape: (1, num_files, emb_dim)\n",
    "                out = agg(embeds)           # Shape: (1, emb_dim)\n",
    "                vec = out.cpu().numpy().reshape(-1).tolist()\n",
    "                writer.writerow([sid[0], json.dumps(vec)])\n",
    "\n",
    "    print(f\"Aggregation completed. Output saved to: {output_csv}\")\n",
    "\n",
    "# ========================================\n",
    "# Main Execution Pipeline\n",
    "# ========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline for CXR embedding extraction and aggregation\n",
    "    \"\"\"\n",
    "    print(\"=== CXR Embedding Generation Pipeline ===\")\n",
    "    \n",
    "    # Load selected subject IDs\n",
    "    print(\"Loading selected subject IDs...\")\n",
    "    subject_ids = pd.read_csv(SUBJECT_IDS_CSV)['subject_id'].astype(int).tolist()\n",
    "    print(f\"Found {len(subject_ids)} selected subjects\")\n",
    "    \n",
    "    # Step 1: Extract individual embeddings from TFRecord files\n",
    "    print(\"\\n--- Step 1: Extracting individual CXR embeddings ---\")\n",
    "    rows, max_files = extract_subject_embeddings(subject_ids)\n",
    "    \n",
    "    if not rows:\n",
    "        print(\"No CXR embeddings found for any subjects. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Save intermediate results\n",
    "    print(\"\\n--- Step 2: Saving intermediate embeddings ---\")\n",
    "    save_intermediate_embeddings(rows, max_files, INTERMEDIATE_CSV)\n",
    "    \n",
    "    # Step 3: Aggregate embeddings using Transformer\n",
    "    print(\"\\n--- Step 3: Aggregating embeddings with Transformer ---\")\n",
    "    aggregate_embeddings(INTERMEDIATE_CSV, OUTPUT_CSV)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n=== Pipeline Completed Successfully ===\")\n",
    "    print(f\"Processed {len(rows)} subjects with CXR data\")\n",
    "    print(f\"Maximum CXR files per subject: {max_files}\")\n",
    "    print(f\"Final aggregated embeddings saved to: {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Clean up intermediate file (optional)\n",
    "    # os.remove(INTERMEDIATE_CSV)\n",
    "    # print(f\"Intermediate file {INTERMEDIATE_CSV} removed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
